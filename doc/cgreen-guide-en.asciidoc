:source-highlighter: highlightjs
:icons: font
:numbered:
:toc:
:pp: ++

= Cgreen - Unit Tests for C and C++
Marcus Baker <marcus@lastcraft.com>; Thomas Nilsson <thomas@junovagen.se>; Jo√£o Freitas <joaohf@gmail.com>;


Cgreen Quickstart Guide
-----------------------

What is Cgreen?
~~~~~~~~~~~~~~~

*Cgreen* is a unit tester for the C and C++ software developer, a test
automation and software quality assurance tool for programmers and
development teams. The tool is completely open source published under
the http://www.gnu.org/licenses/lgpl.html[LGPL]

Unit testing is a development practice popularised by the agile
development community.  It is characterised by writing many small
tests alongside the normal code. Often the tests are written before
the code they are testing, in a tight test-code-refactor loop.  Done
this way, the practice is known as Test Driven Development. *Cgreen*
was designed to support this style of development.

Unit tests are written in the same language as the code, in our case
C or C++. This avoids the mental overhead of constantly switching language,
and also allows you to use any application code in your tests.

Here are some of its features:

- Fluent API resulting in very readable tests
- Expressive and clear output using the default reporter
- Extensible reporting mechanism
- Extensive and expressive constraints for many datatypes
- Fully composable test suites
- BDD-flavoured test declarations with Before and After declarations
- Each test runs in its own process for test suite robustness
- An isolated test can be run in a single process for debugging
- Fully functional mocks, both strict and loose
- Automatic discovery and running of tests using dynamic library inspection

*Cgreen* also supports the classic xUnit-style assertions for easy
 porting from other frameworks.
        
*Cgreen* was initially developed to support C programming, but there
is also excellent support for C++. It was initially a spinoff from a
research project at Wordtracker and created by Marcus Baker.


Cgreen - Vanilla or Chocolate?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Test driven development (TDD) really catched on when the JUnit
framework for Java spread to other langauges, giving us a family of
xUnit tools. *Cgreen* was born in this wave and have many similarities
to the xUnit family.

But TDD evolved over time and modern thinking and practice is more
along the lines of BDD, an acronym for Behaviour Driven Development,
made popular by people like Dan North and frameworks like JBehave,
RSpec, Cucumber and Jasmine.

*Cgreen* follows this trend and has evolved to embrace a BDD-flavoured
style of testing. Although the fundamental mechanisms in TDD and BDD
are much the same, the shift in focus by changing wording from 'tests'
to 'behaviour specifications' is very significant.

This document will present *Cgreen* using the more modern and better
BDD-style. In a later section you can have a peek at the classic TDD
API.

Installing Cgreen
~~~~~~~~~~~~~~~~~

There are two ways to install *Cgreen* in your system.

Installing a package
^^^^^^^^^^^^^^^^^^^^
      
The first way is to use the RPM or DEB package provided by the *Cgreen*
Team. You can fetch it from http://cgreen.sourceforge.net[Cgreen
website project]. Download and install it using the normal procedures
for your system.

NOTE: There is currently no supported packages available.

Installing from source
^^^^^^^^^^^^^^^^^^^^^^

The second way is available for developers and advanced
users. Basically this consists of fetching the sources of the project
on https://github.com/cgreen-devs/cgreen[GitHub] and compiling
them. To do this you need the http://www.cmake.org[CMake] build
system.

When you have the CMake tool, the steps are:

-----------------------------------------
$ tar -zxpvf cgreen.tar.gz
$ make
$ make test    
$ make install
-----------------------------------------

The initial __make__ command will configure the build process and
create a separate `build` directory before going there and building
using *CMake*. This is called an 'out of source build'. It compiles
*Cgreen* from outside the sources directory. This helps the overall
file organization and enables multi-target builds from the same
sources by leaving the complete source tree untouched. The top level
`Makefile` will by default create the two directories, `build/build-c`
and `build/build-c++`, which houses the __C__ and __C++__ builds
respectively.

TIP: Experienced users may tweak the build configurations by going to
the appropriate build subdirectory and use `ccmake ../..` to modify
the build configuration in that subtree. Creating an extra build
subdirectory with a name starting with `build-` is also possible. The
`Makefile` will automatically pick that up and build there too.

The build process will create a library (on unix called `libcgreen.so`
and `libcgreen++.so` for C++) which can be used in conjunction with the
`cgreen.h` header file to compile test code. The created library is
installed in the system, by default in the `/usr/local/lib/`.


Your First Test
^^^^^^^^^^^^^^^

We will first demonstrate the use of *CGreen* by first writing a test
to confirm everything is working. Let's start with a simple test
module with no tests, called `first_test.c`...

[source,c]
---------------------------------------
include::tutorial_src/first_tests0.c[]
---------------------------------------

This is very unexciting. It just creates an empty test suite
and runs it.  It's usually easier to proceed in small steps, though,
and this is the smallest one I could think of. The only complication
is the `cgreen.h` header file and the mysterious looking
"declarations" at the beginning of the file.

The BDD flavoured *Cgreen* notation calls for a Subject Under Test
(SUT), or a 'context'. The declarations give a context to the tests
and it also makes it more natural to talk about which module or class,
the subject under test, is actually responsible for the functionality
we are expressing. In one way we are 'describing', or spec'ing, the
functionality of the SUT. That's what the `Describe();` does. And for
technical reasons (actually requirements of the C language), you must
declare the `BeforeEach()` and `AfterEach()` functions even if they
are empty. (You will get strange errors if you don't!)

I am assuming you have the *Cgreen* folder in the include search
path to ensure compilation works, otherwise you'll need to add that in
the compilation command.

Then, building this test is, of course, trivial...

-----------------------------
$ gcc -c first_test.c
$ gcc first_test.o -lcgreen -o first_test
$ ./first_test
-----------------------------
          
Invoking the executable should give...

-----------------------------
include::tutorial_src/first0.out[]
-----------------------------

All of the above rather assumes you are working in a Unix like
environment, probably with 'gcc'. The code is pretty much standard
C99, so any C compiler should work.  *Cgreen* should compile on all
systems that support the `sys/msg.h` messaging library.  This has been
tested on Linux, MacOSX and Cygwin so far, but not Windows.

So far we have tested compilation, and that the test suite actually runs.
Let's add a meaningless test or two so that you can see how it runs...

[source,c]
-----------------------------
include::tutorial_src/first_tests1.c[]
-----------------------------

A test is denoted by the macro `Ensure`. You can think of a test as
having a `void (void)` signature. You add the test to your suite using
`add_test_with_context()`.

On compiling and running, we now get the output...

-----------------------------
include::tutorial_src/first1.out[]
-----------------------------

The `TextReporter`, created by the `create_text_reporter()` call, is
the easiest way to output the test results. It prints the failures as
intelligent and expressive text messages to your console.

Of course "0" would never equal "1", but this shows how *Cgreen*
presents the expression that you want to assert. We can also see a
handy short hand form for boolean expressions (`assert_that(1 == 1);`).


Five minutes doing TDD with Cgreen
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For a more realistic example we need something to test. We'll pretend
that we are writing a function to split the words of a sentence in
place. It does this by replacing any spaces with string terminators
and returns the number of conversions plus one.  Here is an example of
what we have in mind...

[source,c]
-------------------------------
char *sentence = strdup("Just the first test");
word_count = split_words(sentence);
-------------------------------

`sentence` should now point at "Just\0the\0first\0test". Not an
obviously useful function, but we'll be using it for something more
practical later.

This time around we'll add a little more structure to our
tests. Rather than having the test as a stand alone program, we'll
separate the runner from the test cases.  That way, multiple test
suites of test cases can be included in the `main()` runner file.
This makes it less work to add more tests later.

Here is the, so far empty, test case in `words_test.c`...

[source,c]
-------------------------------
include::tutorial_src/words_tests0.c[]
-------------------------------

Here is the `all_tests.c` test runner...

[source,c]
-------------------------------
include::tutorial_src/all_tests.c[]
-------------------------------

*Cgreen* has two ways of running tests. The default is to run all
tests in their own protected processes. This is what happens if you
invoke `run_test_suite()`. All tests are then completely independent
since they run in separate processes, preventing a single run-away
test from bringing the whole program down with it. It also ensures
that one test cannot leave any state to the next, thus forcing you to
setup the prerequisites for each test correctly and clearly.

But if you want to debug any of your tests the constant `fork()ing`
can make that difficult or impossible. Cygwin is one example of an
environment where debugging forks might be very hard.

To make debugging simpler, *Cgreen* does not fork() when only a single
test is run by name with the function `run_single_test()`. And if you
want to debug, you can obviously set a breakpoint at that test (but
note that its actual name probably have been mangled). But since
*Cgreen* does some book-keeping before actually getting to the test, a
better function is the one simply called `run()`.

Building this scaffolding...

-------------------------------
$ gcc -c words_test.c
$ gcc -c all_tests.c
$ gcc words_test.o all_tests.o -lcgreen -o all_tests
-------------------------------

...and executing the result gives the familiar...

-------------------------------
include::tutorial_src/words0.out[]
-------------------------------

Note that we get an extra level of output here, we have both `main`
and `words_tests`. That's because `all_tests.c` adds the words test
suite to its own (named `main` since it was created in the function
`main()`). All this scaffolding is pure overhead, but from now on
adding tests will be a lot easier.

Here is a first test for `split_words()` in `words_test.c`...

[source,c]
-------------------------------
include::tutorial_src/words_tests1.c[]
-------------------------------

The `assert_that()` macro takes two parameters, the value to assert
and a constraint. The constraints comes in various forms. In this case
we use the probably most common, `is_equal_to()`. With the default
`TextReporter` the message is sent to `STDOUT`.

To get this to compile we need to create the `words.h` header file...

[source,c]
-------------------------------
include::tutorial_src/words.h[lines=1]
-------------------------------

...and to get the code to link we need a stub function in `words.c`...

[source,c]
-------------------------------
include::tutorial_src/words1.c[lines=3..5]
-------------------------------

A full build later...

-------------------------------
$ gcc -c all_tests.c
$ gcc -c words_test.c
$ gcc -c words.c
$ gcc all_tests.o words_test.o words.o -lcgreen -o all_tests
$ ./all_tests
-------------------------------

...and we get the more useful response...

-------------------------------
include::tutorial_src/words1.out[]
-------------------------------

The breadcrumb trail following the "Failure" text is the nesting of
the tests. It goes from the test suites, which can be nested in each
other, through the test function, and finally to the message from the
assertion. In the language of *Cgreen*, a "failure" is a mismatched
assertion, an "exception" occurs when a test fails to complete for any
reason.

We could get this to pass just by returning the value 4. Doing TDD in
really small steps, you would actually do this, but frankly this
example is too simple. Instead we'll go straight to the core of the
implementation...

[source,c]
--------------------------------
include::tutorial_src/words2.c[]
--------------------------------

Running it gives...

---------------------------------
include::tutorial_src/words2.out[]
---------------------------------

There is actually a hidden problem here, but our tests still passed so
we'll pretend we didn't notice.

So it's time to add another test. We want to confirm that the string
is broken into separate words...

[source,c]
---------------------------------
...
include::tutorial_src/words_tests2.c[lines=10]
    ...
include::tutorial_src/words_tests2.c[lines=15..23]
---------------------------------

Sure enough, we get a failure...

----------------------------------
include::tutorial_src/words3.out[]
----------------------------------

Not surprising given that we haven't written the code yet.

The fix...

[source,c]
----------------------------------
include::tutorial_src/words3.c[]
----------------------------------

...reveals our previous hack...

----------------------------------
include::tutorial_src/words4.out[]
----------------------------------

Our earlier test now fails, because we have affected the `strlen()`
call in our loop.  Moving the length calculation out of the loop...

[source,c]
----------------------------------
include::tutorial_src/words4.c[lines=3..5]
    ...
include::tutorial_src/words4.c[lines=10..-1]
----------------------------------

...restores order...
		  
----------------------------------
include::tutorial_src/words5.out[]
----------------------------------

It's nice to keep the code under control while we are actually writing
it, rather than debugging later when things are more complicated.

That was pretty straight forward. Let's do something more interesting.

What are mock functions?
~~~~~~~~~~~~~~~~~~~~~~~~

The next example is a more realistic extension of our previous
attempts. As in real life we first implement something basic and then
we go for the functionality that we need. In this case a function that
invokes a callback for each word found in a sentence. Something
like...

[source,c]
----------------------------------
void act_on_word(const char *word, void *memo) { ... }
words("This is a sentence", &act_on_word, &memo);
----------------------------------

Here the `memo` pointer is just some accumulated data that the
`act_on_word()` callback might work with. Other people will write the
`act_on_word()` function and probably many other functions like
it. The callback is actually a flex point, and not of interest right
now.

The function under test is the `words()` function and we want to make
sure it walks the sentence correctly, dispatching individual words as
it goes. So what calls are made are very important. How to test this?

Let's start with a one word sentence. In this case we would expect the
callback to be invoked once with the only word, right? Here is the
test for that...

[source,c]
---------------------------------
include::tutorial_src/words_tests3.c[lines=1..2]
...
include::tutorial_src/words_tests3.c[lines=27..38]
    ...
include::tutorial_src/words_tests3.c[lines=41..-1]
---------------------------------

What is the funny looking `mock()` function?

A mock is basically a programmable object. In C objects are limited to
functions, so this is a mock function. The macro `mock()` compares the
incoming parameters with any expected values and dispatches messages
to the test suite if there is a mismatch. It also returns any values
that have been preprogrammed in the test.
 
The test function is
`invokes_callback_once_for_single_word_sentence()`. It programs the
mock function using the `expect()` macro. It expects a single call,
and that single call should use the parameters "Word" and `NULL`. If
they don't match, we will get a test failure.

Of course, we only add the test method, not the mock callback, to the
test suite.

For a successful compile and link, the `words.h` file must now look like...

[source,c]
----------------------------
include::tutorial_src/words.h[]
----------------------------

...and the `words.c` file should have the stub...

[source,c]
----------------------------
include::tutorial_src/words5.c[lines=14..15]
----------------------------

This gives us the expected failing test...

----------------------------
include::tutorial_src/words6.out[]
----------------------------

*Cgreen* reports that the callback was never invoked. We can easily get
the test to pass by filling out the implementation with...

[source,c]
----------------------------
include::tutorial_src/words6.c[lines=14..16]
----------------------------

That is, we just invoke it once with the whole string. This is a
temporary measure to get us moving. For now everything should pass,
although it doesn't drive much functionality yet.

----------------------------
include::tutorial_src/words7.out[]
----------------------------

That was all pretty conventional, but let's tackle the trickier case
of actually splitting the sentence. Here is the test function we will
add to `words_test.c`...

[source,c]
----------------------------
include::tutorial_src/words_tests4.c[lines=37..43]
----------------------------

Each call is expected in sequence. Any failures, or left-over calls,
or extra calls, and we get failures. We can see all this when we run
the tests...

----------------------------
include::tutorial_src/words8.out[]
----------------------------

The first failure tells the story. Our little `words()` function
called the mock callback with the entire sentence. This makes sense,
because that was the hack we did to get to the next test.

Although not relevant to this guide, I cannot resist getting these
tests to pass.  Besides, we get to use the function we created
earlier...

[source,c]
-----------------------------
include::tutorial_src/words7.c[lines=15..29]
-----------------------------

And with some work we are rewarded with...

------------------------------
include::tutorial_src/words9.out[]
------------------------------

More work than I like to admit as it took me three goes to get this
right. I firstly forgot the `+ 1` added on to `strlen()`, then forgot
to swap `sentence` for `word` in the `(*callback)()` call, and finally
third time lucky. Of course running the tests each time made these
mistakes very obvious. It's taken me far longer to write these
paragraphs than it has to write the code.


Using Cgreen with C{pp}
~~~~~~~~~~~~~~~~~~~~~~~

The above example, as well as most of this guide, shows how to uses
*CGreen* with C. You can also use *CGreen* with C++. This is actually
quite simple. If you have installed the *Cgreen* library for C{pp} all you
have to do is

  * Use the `cgreen` name space by adding `using namespace cgreen;` at
    the beginning of the file with your tests

  * Use the C{pp} version of the library when linking (`-lcgreen{pp}` or
    something similar)

There is also one extra feature when you use C++, the `assert_throws` function.

Building Cgreen test suites
---------------------------

*Cgreen* is a tool for building unit tests in the C language. These are
usually written alongside the production code by the programmer to
prevent bugs. Even though the test suites are created by software
developers, they are intended to be human readable C code, as part of
their function is an executable specification.  Used in this way, the
test harness delivers constant quality assurance.

In other words you'll get less bugs.

Writing basic tests
~~~~~~~~~~~~~~~~~~~

*Cgreen* tests are simply C, or C++, functions with no parameters and
no return value. To signal that they actually are tests we mark them
with the `Ensure` macro. An example might be...

[source,c]
-----------------------------
include::tutorial_src/strlen_tests1.c[lines=8..10]
-----------------------------

The `Ensure` macro takes two arguments (in the BDD style) where the
first is the Subject Under Test (SUT) which must be declared with the
`Describe` macro.

[source,c]
-----------------------------
include::tutorial_src/strlen_tests1.c[lines=4]
-----------------------------

The second argument is the test name and can be anything you want as
long as it fullfills the rules for an identifier in C and C++. A
typical way to choose the named of the tests is what we see here,
reading the declaration of the test makes sense since it is almost
plain english, "Ensure strlen returns five for 'hello'". No problem
understanding what we aim to test. And it can be viewed as an example
from a description of what strlen should be able to do. In a way,
extracting all the `Ensure`:s from your test might give you all the
documentation you'll need.

The `assert_that()` call is the primary part of an assertion, which is
complemented with a constraint, in this case `is_equal_to()`, as a
parameter. This makes a very fluent interface to the asserts, that
actually reads like English.

NOTE: Sometimes you just want to fail the test explicitly, and there
is a function for that too, `fail_test(const char *message)`. And
there is a function to explicitly pass, `pass_test(void)`.

Assertions send messages to *Cgreen*, which in turn outputs the
results.


The Standard Constraints
~~~~~~~~~~~~~~~~~~~~~~~~

Here are the standard constraints...

|=========================================================
|*Constraint* |*Passes if actual value/expression...*
| `is_true` | evaluates to true
| `is_false` | evaluates to false
| `is_null` | equals null
| `is_non_null` | is a non null value
||
| `is_equal_to(value)` | '== value'
| `is_not_equal_to(value)` | '!= value'
| `is_greater_than(value)` | '> value'
| `is_less_than(value)` | '< value'
||
| `is_equal_to_contents_of(pointer, size)` | matches the data pointed to by `pointer` to a size of `size` bytes
| `is_not_equal_to_contents_of(pointer, size)` | does not match the data pointed to by `pointer` to a size of `size` bytes
||
| `is_equal_to_string(value)` | are equal when compared using `strcmp()`
| `is_not_equal_to_string(value)` | are not equal when compared using `strcmp()`
| `contains_string(value)` | contains `value` when evaluated using `strstr()`
| `does_not_contain_string(value)` | does not contain `value` when evaluated using `strstr()`
| `begins_with_string(value)` | starts with the string `value`
||
| `is_equal_to_double(value)` | are equal to `value` within the number of significant digits (which you can set with a call to `significant_figures_for_assert_double_are(int figures)`)
| `is_not_equal_to_double(value)` | are not equal to `value` within the number of significant digits
| `is_less_than_double(value)` | `< value` withing the number of significant digits
| `is_greater_than_double(value)` | `> value` within the number of significant digits
|=========================================================

The boolean assertion macros accept an `int` value. The equality
assertions accept anything that can be cast to `intptr_t` and simply
perform an `==` operation. The string comparisons are slightly
different in that they use the `<string.h>` library function
`strcmp()`.  If `is_equal_to()` is used on `char *` pointers then the
pointers have to point at the same string to pass.

A cautionary note about the constraints is that you cannot use C/C++
string literal concatenation (like "don't" "use" "string"
"concatenation") in the parameters to the constraints. If you do, you
will get weird error messages about missing arguments to the
constraint macros. This is caused by the macros using argument strings
to produce nice failure messages.


Asserting C++ Exceptions
~~~~~~~~~~~~~~~~~~~~~~~~

When you use *CGreen* with C++ there is one extra assertion available:

|=========================================================
|*Assertion* |*Description*
| `assert_throws(exception, expression)` | Passes if evaluating `expression` throws `exception`
|=========================================================


BDD Style vs. TDD Style
~~~~~~~~~~~~~~~~~~~~~~~

So far we have encouraged the modern BDD style. It has merits that we
really want you to benefit from. But you might come across another
style, the standard TDD style, which is more inline with previous
thinking and might be more similar to other frameworks.

The only difference, in principle, is the use of the SUT or
'context'. In the BDD style you have it, in the TDD style you don't.

[source,c]
.BDD style:
-----------------------------
include::tutorial_src/strlen_tests2.c[lines=4..16]
-----------------------------
<1> The `Describe` macro must name the SUT
<2> The `BeforeEach` function...
<3> ... and the `AfterEach` functions must exist and name the SUT
<4> The test need to name the SUT
<5> Adding to the test suite

CAUTION: You can only have tests for a single SUT in the same source file.

If you use the older pure-TDD style you skip the `Describe` macro, the
`BeforeEach` and `AfterEach` functions. You don't need a SUT in the
`Ensure()` macro or when you add the test to the suite.

[source,c]
.TDD style:
-----------------------------
include::tutorial_src/strlen_tests3.c[lines=3..12]
-----------------------------
<1> No `Describe`, `BeforeEach()` or `AfterEach()`
<2> No SUT/context in the `Ensure()` macro
<3> No SUT/context in `add_test()` and you should use this function instead
of `..with_context()`.

TIP: You might think of the TDD style as the BDD style with a default
SUT or context.

Legacy style assertions
~~~~~~~~~~~~~~~~~~~~~~~

Cgreen have been around for a while, developed and matured. There is
an older style of assertions that was the initial version, a style
that we now call the 'legacy style', because it was more aligned with
the original, now old, unit test frameworks. If you are not interested
in historical artifacts, I recommend that you skip this section.

But for completeness of documentation, here are the legacy style
assertion macros:

|=========================================================
|*Assertion* |*Description*
| `assert_true(boolean)` |Passes if boolean evaluates true
| `assert_false(boolean)` |Fails if boolean evaluates true
| `assert_equal(first, second)` |Passes if 'first == second'
| `assert_not_equal(first, second)` |Passes if 'first != second'
| `assert_string_equal(char *, char *)` |Uses 'strcmp()' and passes if the strings are equal
| `assert_string_not_equal(char *, char *)` |Uses 'strcmp()' and fails if the strings are equal
|=========================================================

Each assertion has a default message comparing the two values. If you
want to substitute your own failure messages, then you must use the
`*_with_message()` counterparts...

|=========================================================
|*Assertion*
| `assert_true_with_message(boolean, message, ...)`
| `assert_false_with_message(boolean, message, ...)`
| `assert_equal_with_message(tried, expected, message, ...)`
| `assert_not_equal_with_message(tried, unexpected, message, ...)`
| `assert_string_equal_with_message(char *, char *, message, ...)`
| `assert_string_not_equal_with_message(char *, char *, message, ...)`
|=========================================================

All these assertions have an additional `char *` message parameter,
which is the message you wished to display on failure. If this is set
to `NULL`, then the default message is shown instead. The most useful
assertion from this group is `assert_true_with_message()` as you can
use that to create your own assertion functions with your own
messages.

Actually the assertion macros have variable argument lists. The
failure message acts like the template in `printf()`. We could change
the test above to be...

[source,c]
-----------------------------
include::tutorial_src/strlen_tests4.c[lines=4..8]
-----------------------------

This should produce a slightly more user friendly message when things
go wrong. But, actually, Cgreens default messages are so good that you
are encouraged to skip the legacy style and go for the more modern
constraints style assertions. Particularly in conjuction with the BDD
style test notation.

IMPORTANT: We strongly recommend the use of BDD Style notation with
constraints based assertions.

A runner
~~~~~~~~

The tests are only run through running a test suite in some form. We
can create and run one especially for this test like so... (But see
also <<runner, Automatic Test Discovery>>.)

[source,c]
-----------------------------
include::tutorial_src/strlen_tests5.c[lines=12..16]
-----------------------------

In case you have spotted that the reference to
`returns_five_for_hello` should have an ampersand in front of it,
`add_test_with_context()` is actually a macro. The `&` is added
automatically. Further more, the `Ensure()`-macro actually mangles the
tests name, so it is not actually a function name. (This might also
make them a bit difficult to find in the debugger....)

To run the test suite, we call `run_test_suite()` on it. So we can
just write...

[source,c]
-----------------------------
include::tutorial_src/strlen_tests5.c[lines=19]
-----------------------------

The results of assertions are ultimately delivered as passes and
failures to a collection of callbacks defined in a `TestReporter`
structure. There is a predefined `TestReporter` in *Cgreen* called the
`TextReporter` that delivers messages in plain text like we have
already seen.

The return value of `run_test_suite()` is a standard C library/Unix
exit code that can be returned directly by the `main()` function.

The complete test code now looks like...

[source,c]
-----------------------------
include::tutorial_src/strlen_tests5.c[]
-----------------------------

Compiling and running gives...

-----------------------------
$ gcc -c strlen_test.c
$ gcc strlen_test.o -lcgreen -o strlen_test
$ ./strlen_test
include::tutorial_src/strlen2.out[]
-----------------------------

We can see that the outer test suite is called `our_tests` since it
was in `our_tests()` we created the test suite. There are no messages
shown unless there are failures. So, let's break our test to see it...

[source,c]
-----------------------------
include::tutorial_src/strlen_tests6.c[lines=8..10]
-----------------------------

...we'll get the helpful message...

-----------------------------
include::tutorial_src/strlen6.out[]
-----------------------------
                
*Cgreen* starts every message with the location of the test failure so
that the usual error message identifying tools (like Emacs's
`next-error`) will work out of the box.

Once we have a basic test scaffold up, it's pretty easy to add more
tests. Adding a test of `strlen()` with an empty string for example...

[source,c]
-----------------------------
...
include::tutorial_src/strlen_tests7.c[lines=12..21]
...
-----------------------------

And so on.

BeforeEach and AfterEach
~~~~~~~~~~~~~~~~~~~~~~~~

It's common for test suites to have a lot of duplicate code,
especially when setting up similar tests. Take this database code for
example...

[source,c]
------------------------------
include::tutorial_src/schema_tests1.c[]
------------------------------

We have already factored out the duplicate code into its own functions
`create_schema()` and `drop_schema()`, so things are not so bad. At
least not yet. But what happens when we get dozens of tests? For a
test subject as complicated as a database
http://www.martinfowler.com/eaaCatalog/activeRecord.html[ActiveRecord],
having dozens of tests is very likely.

We can get *Cgreen* to do some of the work for us by calling these
methods before and after each test in the test suite.
 
Here is the new version...

[source,c]
---------------------------
...
include::tutorial_src/schema_tests2.c[lines=6] 
    ...
include::tutorial_src/schema_tests2.c[lines=11..13]
    ...
include::tutorial_src/schema_tests2.c[lines=18..41]
...
---------------------------

With this new arrangement *Cgreen* runs the `create_schema()` function
before each test, and the `drop_schema()` function after each
test. This saves some repetitive typing and reduces the chance of
accidents. It also makes the tests more focused.

The reason we try so hard to strip everything out of the test
functions is the fact that the test suite acts as documentation. In
our `person.h` example we can easily see that `Person` has some kind
of name property, and that this value must be unique. For the tests to
act like a readable specification we have to remove as much mechanical
clutter as we can.

In this particular case there are more lines that we could move from
the tests to `BeforeEach()`:

[source,c]
---------------------------
include::tutorial_src/schema_tests2.c[lines=25..26] 
---------------------------

Of course that would require an extra variable, and it might make the
tests less clear. And as we add more tests, it might turn out to not
be common to all tests. This is a typical judgement call that you
often get to make with `BeforeEach()` and `AfterEach()`.

NOTE: If you use the pure-TDD notation, not having the test subject
named by the `Describe` macro, you can't have the `BeforeEach()` and
`AfterEach()` either. In this case you can still run a function before
and after every test. Just nominate any `void(void)` function by
calling the function `set_setup()` and/or `set_teardown()` with the
suite and the function that you want to run before/after each test,
e.g. in the example above `set_setup(suite, create_schema);` and
`set_teardown(suite, drop_schema);`.

A couple of details. There is only one `BeforeEach()` and one
`AfterEach()` allowed in each `TestSuite`. Also, the `AfterEach()`
function may not be run if the test crashes, causing some test
interference. This brings us nicely onto the next section...


Each test in its own process
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider this test method...

[source,c]
-----------------------------
include::tutorial_src/crash_tests1.c[lines=8..11]
-----------------------------

Crashes are not something you would normally want to have in a test
run. Not least because it will stop you receiving the very test output
you need to tackle the problem.

To prevent segmentation faults and other problems bringing down the
test suites, *Cgreen* runs every test in its own process.

Just before calling the `BeforeEach()` (or `setup`) function, *Cgreen*
`fork()`:s. The main process waits for the test to complete normally
or die. This includes the calling the `AfterEach()`(or `teardown`)
function, if any. If the test process dies, an exception is reported
and the main test process carries on.

For example...

[source,c]
-----------------------------
include::tutorial_src/crash_tests1.c[]
-----------------------------

When built and run, this gives...

-----------------------------
include::tutorial_src/crash1.out[]
-----------------------------

The obvious thing to do now is to fire up the debugger. Unfortunately,
the constant `fork()`:ing of *Cgreen* can be an extra complication too
many when debugging. It's enough of a problem to find the bug.

To get around this, and also to allow the running of one test at a
time, *Cgreen* has the `run_single_test()` function. The signatures of
the two run methods are...

- `int run_test_suite(TestSuite *suite, TestReporter *reporter);`
- `int run_single_test(TestSuite *suite, char *test, TestReporter *reporter);`

The extra parameter of `run_single_test()`, the `test` string, is the
name of the test to select.  This could be any test, even in nested
test suites (see below). Here is how we would use it to debug our
crashing test...

[source,c]
-----------------------------
include::tutorial_src/crash_tests2.c[lines=13..17]
-----------------------------

When run in this way, *Cgreen* will not `fork()`.

TIP: The function `run()` is a good place to place a breakpoint.

The following is a typical session:

-----------------------------------
$ gdb crash2
...
(gdb) break main
(gdb) run
...
(gdb) break run
(gdb) continue
...
Running "main" (1 tests)...

Breakpoint 2, run_the_test_code (suite=suite@entry=0x2003abb0,
    spec=spec@entry=0x402020 <CgreenSpec__CrashExample__seg_faults_for_null_dereference__>,
    reporter=reporter@entry=0x2003abe0) at /cygdrive/c/Users/Thomas/Utveckling/Cgreen/cgreen/src/runner.c:270
270         run(spec);
(gdb) step
run (spec=0x402020 <CgreenSpec__CrashExample__seg_faults_for_null_dereference__>)
    at /cygdrive/c/Users/Thomas/Utveckling/Cgreen/cgreen/src/runner.c:217
217             spec->run();
(gdb) step
CrashExample__seg_faults_for_null_dereference () at crash_test2.c:9
9           int *p = NULL;
(gdb) step
10          (*p)++;
(gdb) step

Program received signal SIGSEGV, Segmentation fault.
0x004011ea in CrashExample__seg_faults_for_null_dereference () at crash_test2.c:10
10          (*p)++;
-----------------------------------

Which shows exactly where the problem is.

This deals with the case where your code throws an exception like
segmentation fault, but what about a process that fails to complete by
getting stuck in a loop?

Well, *Cgreen* will wait forever too. But, using the C signal
handlers, we can place a time limit on the process by sending it an
interrupt. To save us writing this ourselves, *Cgreen* includes the
`die_in()` function to help us out.

Here is an example of time limiting a test...

[source,c]
---------------------------
...
include::tutorial_src/crash_tests3.c[lines=8]
    ...
include::tutorial_src/crash_tests3.c[lines=11..23]
---------------------------
                
When executed, the code will slow for a second, and then finish with...

---------------------------
include::tutorial_src/crash3.out[]
---------------------------

Note that you see the test results as they come in. *Cgreen* streams the
results as they happen, making it easier to figure out where the test
suite has problems.

Of course, if you want to set a general time limit on all your tests,
then you can add a `die_in()` to a `BeforeEach()` (or `setup()`)
function. *Cgreen* will then apply the limit to each of the tests in
that context, of course.

Another possibility is the use of an environment variable named
`CGREEN_TIMEOUT_PER_TEST` which, if set to a number will apply that
timeout to every test run. This will apply to all tests in the same
run.

Building composite test suites
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            
The `TestSuite` is a composite structure.  This means test suites can
be added to test suites, building a tree structure that will be
executed in order.

Let's combine the `strlen()` tests with the `Person` tests above.
Firstly we need to remove the `main()` calls.  E.g...

[source,c]
----------------------------
include::tutorial_src/suite_strlen_tests.c[lines=8]
   ...
include::tutorial_src/suite_strlen_tests.c[lines=10..12]
   ...
include::tutorial_src/suite_strlen_tests.c[lines=14..-1]
----------------------------

Then we can write a small runner with a new `main()` function...

[source,c]
-----------------------
include::tutorial_src/suite1.c[]
-----------------------

It's usually easier to place the `TestSuite` prototypes directly in
the runner source, rather than have lot's of header files.  This is
the same reasoning that let us drop the prototypes for the test
functions in the actual test scripts.  We can get away with this,
because the tests are more about documentation than encapsulation.

As we saw above, we can run a single test using the
`run_single_test()` function, and we'd like to be able to do that from
the command line. So we added a simple `if` block to take the test
name as an optional argument.  The entire test suite will be searched
for the named test.  This trick also saves us a recompile when we
debug.

When you use the BDD notation you can only have a single test subject
(which is actually equivalent of a suite) in a single file because you
can only have one `Describe()` macro in each file. But using this
strategy you can create composite suites that takes all your tests and
run them in one go.

CAUTION: Rewrite pending. The next couple section does not reflect the
current best thinking. They are remnants of the TDD notation. Using
BDD notation you would create separate contexts, each in its own file,
with separate names, for each of the fixture cases.

NOTE: If you use the TDD (non-BDD) notation you can build several test
suites in the same file, even nesting them.  We can even add mixtures
of test functions and test suites to the same parent test suite.
Loops will give trouble, however.

NOTE: If we do place several suites in the same file, then all the suites
will be named the same in the breadcrumb trail in the test message.
They will all be named after the function the create call sits in.  If
you want to get around this, or you just like to name your test
suites, you can use `create_named_test_suite()` instead of
`create_test_suite()`.  This takes a single string parameter.  In fact
`create_test_suite()` is just a macro that inserts the `__func__`
constant into `create_named_test_suite()`.

What happens to `setup` and `teardown` functions in a `TestSuite` that
contains other `TestSuite`:s?

Well firstly, *Cgreen* does not `fork()` when running a suite.  It
leaves it up to the child suite to `fork()` the individual tests.
This means that a `setup` and `teardown` will run in the main
process.  They will be run once for each child suite.

We can use this to speed up our `Person` tests above.  Remember we
were creating a new connection and closing it again in the fixtures.
This means opening and closing a lot of connections.  At the slight
risk of some test interference, we could reuse the connection accross
tests...

[source,c]
-----------------------
...
static MYSQL *connection;

static void create_schema() {
    mysql_query(connection, "create table people (name, varchar(255) unique)");
}

static void drop_schema() {
    mysql_query(connection, "drop table people");
}

Ensure(can_add_person_to_database) { ... }
Ensure(cannot_add_duplicate_person) { ... }

void open_connection() {
    connection = mysql_init(NULL);
    mysql_real_connect(connection, "localhost", "me", "secret", "test", 0, NULL, 0);
}

void close_connection() {
    mysql_close(connection);
}

TestSuite *person_tests() {
    TestSuite *suite = create_test_suite();
    set_setup(suite, create_schema);
    set_teardown(suite, drop_schema);
    add_test(suite, can_add_person_to_database);
    add_test(suite, cannot_add_duplicate_person);

    TestSuite *fixture = create_named_test_suite("Mysql fixture");
    add_suite(fixture, suite);
    set_setup(fixture, open_connection);
    set_teardown(fixture, close_connection);
    return fixture;
}
-----------------------

The trick here is creating a test suite as a wrapper whose sole
purpose to wrap the main test suite in the fixture.  This is our
'fixture' pointer.  This code is a little confusing, because we have
two sets of fixtures in the same test script.

We have the MySQL connection fixture.  This is runs
`open_connection()` and `close_connection()` just once at the
beginning and end of the person tests.  This is because the `suite`
pointer is the only member of `fixture`.

We also have the schema fixture, the `create_schema()` and
`drop_schema()`, which is run before and after every test.  Those are
still attached to the inner `suite`.

In the real world we would probably place the connection
fixture in its own file...

[source,c]
-----------------------
static MYSQL *connection;

MYSQL *get_connection() {
    return connection;
}

static void open_connection() {
    connection = mysql_init(NULL);
    mysql_real_connect(connection, "localhost", "me", "secret", "test", 0, NULL, 0);
}

static void close_connection() {
    mysql_close(connection);
}

TestSuite *connection_fixture(TestSuite *suite) {
    TestSuite *fixture = create_named_test_suite("Mysql fixture");
    add_suite(fixture, suite);
    set_setup(fixture, open_connection);
    set_teardown(fixture, close_connection);
    return fixture;
}
-----------------------

This allows the reuse of common fixtures across projects.
             
[[runner, Automatic Test Discovery]]

Automatic Test Discovery
------------------------

Forgot to add your test?
~~~~~~~~~~~~~~~~~~~~~~~~

When we write a new test we focus on the details about the test we are
trying to write. And writing tests is no trivial matter so this might
well take a lot of brain power.

So, it comes as no big surprise, that sometimes you write your test
and then forget to add it to the suite. When we run it it appears that it
passed on the first try! Although this *should* really make you
suspicious, sometimes you get so happy that you just continue with
churning out more tests and more code. It's not until some (possibly
looong) time later that you realize, after much headache and
debugging, that the test did not actually pass. It was never even run!

There are practices to minimize the risk of this happening, such as
always running the test as soon as you can set up the test. This way
you will see it fail before trying to get it to pass.

But it is still a practice, something we, as humans, might fail to do
at some point. Usually this happens when we are most stressed and in
need of certainty.

The solution - the 'cgreen-runner'
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

*Cgreen* gives you a tool to avoid not only the risk of this
happening, but also the extra work and extra code. It is called the
`cgreen-runner`.

The `cgreen-runner` should come with your *Cgreen* installation if
your platform supports the technique that is required, which is
'programatic access to dynamic loading of libraries'. This means
that a program can load an external library of code into memory and
inspect it. Kind of self-inspection, or reflexion.

So all you have to do is to build a dynamically loadable library of
all tests (and of course your objects under test and other necessary
code). Then you can run the `cgreen-runner` and point it to the
library. The runner will then load the library, enumerate all tests in
it, and run every test.

It's automatic, and there is nothing to forget.

Using the runner
~~~~~~~~~~~~~~~~

Assuming your tests are in `first_test.c` the typical command to
build your library using `gcc` would be

--------------------------
$ gcc -shared -o first_test.so -fPIC first_test.c -lcgreen
--------------------------

The `-fPIC` means to generate 'position independent code' which is
required if you want to load the library dynamically.

How to build a dynamically loadable shared library might vary a lot
depending on your platform. Can't really help you there, sorry!

As soon as we have linked it we can run the tests using the
`cgreen-runner` by just giving it the shared, dynamic loadable, object
library as an argument:

-------------------------
$ cgreen-runner first_test.so
include::tutorial_src/runner1.out[]
-------------------------

More or less exactly the same output as when we ran our first test in
the beginning of this quickstart tutorial. We can see that the top
level of the tests will be named as the library it was discovered in,
and the second level is the context for our Subject Under Test, in
this case 'Cgreen'. We also see that the context is mentioned in the
failure message, giving a fairly obvious 'Cgreen -> fails_this_test'.

Now we can actually delete the main function in our source code. We
don't need all this:

[source,c]
------------------------
include::tutorial_src/first_tests1.c[lines=15..20]
------------------------

It always feel good to delete code, right?

We can also select which test to run:

-------------------------
$ cgreen-runner first_test.so Cgreen:this_test_should_fail
include::tutorial_src/runner2.out[]
-------------------------

As the runner requires the BDD notation to discover tests, we also
need to indicate which context the test we want to run is in. In this
case `Cgreen` so the test should be refered to as
`Cgreen:this_test_should_fail`.


[[options, Cgreen Runner Options]]

Cgreen Runner Options
~~~~~~~~~~~~~~~~~~~~~

Once you get the build set up right for the cgreen-runner everything
is fairly straight-forward. But you have a few options:

--xml <prefix>:: Instead of messages on stdout with the TextReporter,
      		     write results into one XML-file per suite or context,
		         compatible with Hudson/Jenkins CI. The filename(s)
		         will be `<prefix>-<suite>.xml`
--suite <name>:: Name the top level suite
--no-run::       Don't run the tests
--verbose::      Show progress information and list discovered tests
--colours::      Use colours (or colors) to emphasis result (requires ANSI-capable terminal)
--quiet::        Be more quiet

The `verbose` option is particularly handy since it will give you the
actual names of all tests discovered. That means if you have long test
names you can avoid mistyping them by copy and paste from the output
of `cgreen-runner --verbose`. It will also give the mangled name of
the test which should make it easier to find in the debugger. Here's
an example:

------------------------
include::tutorial_src/runner3.out[]
------------------------


You can also run tests in multiple libraries in one go by adding them
to the `cgreen-runner` command:

-----------------------
$ cgreen-runner first_set.so second_set.so ...
-----------------------

Setup, Teardown and custom reporters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The cgreen-runner will only run setup and teardown functions if you
use the BDD-ish style with `BeforeEach()` and `AfterEach()` as
described above. The runner does not pickup `setup()` and `teardown()`
added to suites, because it actually doesn't run suites. It discovers
all tests and runs them one by one. The macros required by the BDD-ish
style ensures that the corresponding `BeforeEach()` and `AfterEach()`
are run before and after each test.

NOTE: The `cgreen-runner` __will__ discover your tests in a shared library
even if you don't use the BDD-ish style. But it will not run the
`setup()` and/or `teardown()` attached to your suite(s). But in case
you have non-BDD style tests __without__ any `setup()` and/or
`teardown()` you can still use the runner. The default suite/context
where the tests live in this case is called `default`. But why don't
you convert your tests to BDD notation? This would save you from the
frustrating trouble-shooting that you risk when you added `setup()`
and `teardown()` and can't understand why they are not run...

So, the runner requires you to use the BDD notation. But since we
recommend that you do anyway, that's no extra problem if you are
starting out from scratch. But see <<changing_style, Changing Style>>
for some easy tips on how to get you there if you already have non-BDD
tests.

You can choose between the TextReporter, which we have been seeing so
far, and the built-in JUnit/Ant compatible XML-reporter using the
`--xml` option. But, it is not possible to use custom reporters as
outlined in <<reporter, Changing Cgreen Reporting>> with the runner.

If you require another custom reporter you need to resort to the
standard, programatic, way of invoking your tests. For now...


Mocking functions with Cgreen
-----------------------------
        
When testing you want certainty above all else.  Random events destroy
confidence in your test suite and force needless extra runs "to be
sure".  A good test places the subject under test into a tightly
controlled environment.  A test chamber if you like.  This makes the
tests fast, repeatable and reliable.

To create a test chamber for testing code, we have to control any
outgoing calls from the code under test.  We won't believe our test
failure if our code is making calls to the internet for example.  The
internet can fail all by itself.  Not only do we not have total
control, but it means we have to get dependent components working
before we can test the higher level code.  This makes it difficult to
code top down.

The solution to this dilemma is to write stub code for the components
whilst the higher level code is written.  This pollutes the code base
with temporary code, and the test isolation disappears when the system
is eventually fleshed out.

The ideal is to have minimal stubs written for each individual test.
*Cgreen* encourages this approach by making such tests easier to write.
         
The problem with streams
~~~~~~~~~~~~~~~~~~~~~~~~
            
How would we test the following code...?

[source,c]
-----------------------
include::tutorial_src/read_paragraph1.c[lines=4..-1]
-----------------------

This is a fairly generic stream filter that turns the incoming
characters into C string paragraphs. Each call creates one paragraph,
returning a pointer to it or returning `NULL` if there is no
paragraph. The paragraph has memory allocated to it and the stream is
advanced ready for the next call. That's quite a bit of functionality,
and there are plenty of nasty boundary conditions. I really want this
code tested before I deploy it.

The problem is the stream dependency. We could use a real stream, but
that will cause all sorts of headaches. It makes the test of our
paragraph formatter dependent on a working stream.  It means we have
to write the stream first, bottom up coding rather than top down.  It
means we will have to simulate stream failures - not easy.  It will
also mean setting up external resources. This is more work, will run
slower, and could lead to spurious test failures.

By contrast, we could write a simulation of the stream for each test,
called a "server stub".

For example, when the stream is empty nothing should happen.  We
hopefully get `NULL` from `read_paragraph` when the stream is
exhausted.  That is, it just returns a steady stream of `EOF`s.

[source,c]
-----------------------
include::tutorial_src/stream_tests0.c[lines=6..22]
-----------------------

Our simulation is easy here, because our fake stream returns only one
value.  Things are harder when the function result changes from call
to call as a real stream would.  Simulating this would mean messing
around with static variables and counters that are reset for each
test.  And of course, we will be writing quite a few stubs. Often a
different one for each test. That's a lot of clutter.

*Cgreen* can handle this clutter for us by letting us write a single
programmable function for all our tests.
             
Record and playback
~~~~~~~~~~~~~~~~~~~
            
We can redo our example by creating a `stream_stub()` function. We can
call it anything we want, and since I thought we wanted to have a
stubbed stream...

[source,c]
-----------------------
include::tutorial_src/stream_tests1.c[lines=6..8]
-----------------------

Hardly longer that our trivial server stub above, it is just a macro
to generate a return value, but we can reuse this in test after
test. Let's see how.

For our simple example above we just tell it to always return `EOF`...

[source,c]
-----------------------
include::tutorial_src/stream_tests1.c[lines=1..17]
-----------------------

The `always_expect()` macro at (1) takes as arguments the function
name and defines the return value using the call to
`will_return()`. This is a declaration of an expectation of a call to
the stub, and we have told our `stream_stub()` to always return `EOF`
when called.

Let's see if our production code actually works...

-----------------------
include::tutorial_src/stream1.out[]
-----------------------

So far, so good.  On to the next test.

If we want to test a one character line, we have to send the
terminating `EOF` or `"\n"` as well as the single character.
Otherwise our code will loop forever, giving an infinite line of that
character.


Here is how we can do this...

[source,c]
-----------------------
include::tutorial_src/stream_tests2.c[lines=19..25]
-----------------------

Unlike the `always_expect()` instruction, `expect()` sets up an
expectation of a single call and specifying `will_return()` sets the
single return value for just that call.  It acts like a record and
playback model.  Successive expectations map out the return sequence
that will be given back once the test proper starts.

We'll add this test to the suite and run it...

-----------------------
include::tutorial_src/stream2.out[]
-----------------------

Oops. Our code under test doesn't work. Already we need a fix...

[source,c]
-----------------------
include::tutorial_src/read_paragraph2.c[lines=4..19]
-----------------------

After which everything is fine...

-----------------------
include::tutorial_src/stream3.out[]
-----------------------
             
How do the *Cgreen* stubs work?  Each `expect()` describes one call to
the stub and the calls to `will_return()` build up a static list of
return values which are used and returned in order as those calls
arrive. The return values are cleared between tests.

The `mock()` macro captures the parameter names and the `__func__`
property (the name of the stub function).  *Cgreen* can then use these
to look up entries in the return list, and also to generate more
helpful messages.

We can crank out our tests quite quickly now...

[source,c]
-----------------------
include::tutorial_src/stream_tests3.c[lines=27..33]
-----------------------

I've been a bit naughty.  As each test runs in its own process, I
haven't bothered to free the pointers to the paragraphs.  I've just
let the operating system do it.  Purists may want to add the extra
clean up code.

I've also used `always_expect()` for the last instruction.  Without
this, if the stub is given an instruction it does not expect, it will
throw a test failure.  This is overly restrictive, as our
`read_paragraph()` function could quite legitimately call the stream
after it had run off of the end.  OK, that would be odd behaviour, but
that's not what we are testing here.  If we were, it would be placed
in a test of its own.  The `always_expect()` call tells *Cgreen* to
keep going after the first three letters, allowing extra calls.

As we build more and more tests, they start to look like a
specification of the wanted behaviour...

[source,c]
-----------------------
include::tutorial_src/stream_tests4.c[lines=35..41]
-----------------------

...and just for luck...

[source,c]
-----------------------
include::tutorial_src/stream_tests4.c[lines=43..46]
-----------------------

This time we musn't use `always_return()`. We want to leave the stream
where it is, ready for the next call to `read_paragraph()`. If we call
the stream beyond the line ending, we want to fail.
             
Oops, that was a little too fast. Turns out we are failing anyway...

-----------------------
include::tutorial_src/stream5.out[]
-----------------------

Clearly we are passing through the line ending.
Another fix later...

[source,c]
-----------------------
include::tutorial_src/read_paragraph3.c[lines=4..20]
-----------------------

And we are passing again...

-----------------------
include::tutorial_src/stream6.out[]
-----------------------
             
There are no limits to the number of stubbed methods within a test,
only that two stubs cannot have the same name. The following will
cause problems...

[source,c]
-----------------------
include::tutorial_src/multiple_streams1.c[lines=10..-1]
-----------------------

You __could__ program the same stub to return values for the two
streams, but that would make a very brittle test. Since we'd be making
it heavily dependent on the exact internal behaviour that we are
trying to test, or test drive, it will break as soon as we change that
implementation. The test will also become very much harder to read and
understand. And we really don't want that.

So, it will be necessary to have two stubs to make this test behave,
but that's not a problem...

[source,c]
-----------------------
include::tutorial_src/multiple_streams2.c[lines=10..-1]
-----------------------

We now have a way of writing fast, clear tests with no external
dependencies. The information flow is still one way though, from stub
to the code under test. When our code calls complex procedures, we
won't want to pick apart the effects to infer what happened. That's
too much like detective work. And why should we? We just want to
know that we dispatched the correct information down the line.

Things get more interesting when we think of the traffic going the
other way, from code to stub. This gets us into the same territory as
mock objects.
             
Setting expectations on mock functions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            
To swap the traffic flow, we'll look at an outgoing example instead.
Here is the prewritten production code...

[source,c]
-----------------------
include::tutorial_src/stream.c[lines=23..32]
-----------------------

This is the start of a formatter utility.  Later filters will probably
break the paragaphs up into justified text, but right now that is all
abstracted behind the `void write(void *, char *)` interface.  Our
current interests are: does it loop through the paragraphs, and does
it crash?

We could test correct paragraph formation by writing a stub that
collects the paragraphs into a `struct`.  We could then pick apart
that `struct` and test each piece with assertions.  This approach is
extremely clumsy in C.  The language is just not suited to building
and tearing down complex edifices, never mind navigating them with
assertions.  We would badly clutter our tests.

Instead we'll test the output as soon as possible, right in
the called function...

[source,c]
-----------------------
...
include::tutorial_src/formatter_tests0.c[lines=12..-1]
...
-----------------------

By placing the assertions into the mocked function, we keep the tests
minimal.  The catch with this method is that we are back to writing
individual functions for each test.  We have the same problem as we
had with hand coded stubs.

Again, *Cgreen* has a way to automate this.  Here is the rewritten
test...

[source,c]
-----------------------
include::tutorial_src/formatter_tests1.c[lines=10..-1]
-----------------------

Where are the assertions?

Unlike our earlier stub, `reader()` can now check its parameters.  In
object oriented circles, an object that checks its parameters as well
as simulating behaviour is called a mock object.  By analogy
`reader()` is a mock function, or mock callback.

Using the `expect` macro, we have set up the expectation that
`writer()` will be called just once.  That call must have the string
`"a"` for the `paragraph` parameter.  If the actual value of that
parameter does not match, the mock function will issue a failure
straight to the test suite.  This is what saves us writing a lot of
assertions.

When specifying behavior of mocks there are three parts. First, how
often the specified behaviour or expectation will be executed:

|=======================================================================
|*Macro*                       |*Description*
|`expect(function, ...)`       |Expected once, in order
|`always_expect(function, ...)`|Expect this behavior from here onwards
|`never_expect(function)`      |From this point this mock function must never be called
|=======================================================================

You can specify constraints and behaviours for each expectation
(except for `never_expect()` naturally). A constraint places
restrictions on the parameters (and will tell you if the expected
restriction was not met), and a behaviour specifies what the mock
should do if the parameter constraints are met.

A parameter constraint is defined using the `when(parameter,
constraint)` macro. It takes two parameters:

|=================================================
|*Parameter* |*Description*
|`parameter` |The name of the parameter to the mock function
|`constraint`|A constraint placed on that parameter
|=================================================

There is a multitude of constraints available (actually, exactly the
same as for the assertions we saw earlier):

|==========================================================================
|*Constraint*                                             |*Type*
|`is_equal_to(value)`                                     | Integers
|`is_equal_to_hex(value)`                                 | Integers
|`is_not_equal_to(value)`                                 | Integers
|`is_greater_than(value)`                                 | Integers
|`is_less_than(value)`                                    | Integers
|                                                         |
|`is_equal_to_contents_of(pointer, size_of_contents)`     |Bytes/Structures
|`is_not_equal_to_contents_of(pointer, size_of_contents)` |Bytes/Structures
||
|`is_equal_to_string(value)`                              |String
|`is_not_equal_to_string(value)`                          |String
|`contains_string(value)`                                 |String
|`does_not_contain_string(value)`                         |String
|`begins_with_string(value)`                              |String
||
|`is_equal_to_double(value)`                              |Double
|`is_not_equal_to_double(value)`                          |Double
|`is_less_than_double(value)`                             |Double
|`is_greater_than_double(value)`                          |Double
|==========================================================================

For the double valued constraints you can set the number of
significant digits to consider a match with a call to
`significant_figures_for_assert_double_are(int figures)`.

Then there are two ways to return results:

|=================================================================================================
|*Macro*                                                      |*Description*
|`will_return(value)`                                         |Return the value from the mock function (which needs to be declared returning that type
|`will_set_contents_of_parameter(parameter_name, value, size)`|Writes the value in the referenced parameter
|=================================================================================================

You can combine these in various ways:

[source,c]
-----------------------
  expect(mocked_file_writer,
        when(data, is_equal_to(42)),
        will_return(EOF));
  expect(mocked_file_reader,
        when(file, is_equal_to_contents_of(&FD, sizeof(FD))),
        when(input, is_equal_to_string("Hello world!"),
        will_set_contents_of_parameter(status, FD_CLOSED, sizeof(bool))));
-----------------------

If multiple `when()` are specified they all need to be fullfilled. You
can of course only have one for each of the parameters of your mock
function.

You can also have multiple `will_set_contents_of_parameter()` in an
expectation, one for each reference parameter, but naturally only one
`will_return()`.

It's about time we actually ran our test...

-----------------------
include::tutorial_src/formatter1.out[]
-----------------------

Confident that a single character works, we can further specify the
behaviour.  Firstly an input sequence...

[source,c]
-----------------------
include::tutorial_src/formatter_tests2.c[lines=25..34]
-----------------------

A more intelligent programmer than me would place all these calls in a
loop.

-------------------------
include::tutorial_src/formatter2.out[]
-------------------------

Next, checking an output sequence...

[source,c]
-----------------------
include::tutorial_src/formatter_tests3.c[lines=36..-1]
-----------------------

Again we can se that the `expect()` calls follow a record and playback
model.  Each one tests a successive call.  This sequence confirms that
we get `"a"`, `"b"` and `"c"` in order.

-----------------------
include::tutorial_src/formatter3.out[]
-----------------------

So, why the 5 passes? Each `expect()` with a constrait is actually
an assert. It asserts that the call specified is actually made with
the parameters given and in the specified order. In this case all the
expected calls were made.

Then we'll make sure the correct stream pointers are passed to the
correct functions.  This is a more realistic parameter check...

[source,c]
-----------------------
include::tutorial_src/formatter_tests4.c[lines=49..-1]
-----------------------

-----------------------
include::tutorial_src/formatter4.out[]
-----------------------

And finally we'll specify that the writer is not called if
there is no paragraph.

[source,c]
-----------------------
include::tutorial_src/formatter_tests5.c[lines=56..-1]
-----------------------

This last test is our undoing...

-----------------------
include::tutorial_src/formatter5.out[]
-----------------------

Obviously blank lines are still being dispatched to the `writer()`.
Once this is pointed out, the fix is obvious...

[source,c]
-----------------------
include::tutorial_src/stream2.c[lines=23..-1]
-----------------------

Tests with `never_expect()` can be very effective at uncovering subtle
bugs.

-----------------------
include::tutorial_src/formatter6.out[]
-----------------------

All done.

=== Mocks Are...

Using mocks is a very handy way to isolate a unit and catch and
control calls to external units. Depending on your style of coding two
schools of thinking have emerged. And of course *Cgreen* supports
both!


==== Strict or Loose Mocks ====

The two schools are thinking a bit differently about what mock
expectations means. Does it mean that all external calls must be
declared and expected? What happens if a call was made to a mock that
wasn't expected? And vice versa, if an expected call was not made?

Actually, the thinking is not only a school of thought, but you might
want to switch from one to the other. So *Cgreen* allows for that too.

By default *Cgreen* mocks are 'strict', which means that a call to
an non-expected mock will be considered a failure. So will an expected
call that was not fullfilled. You might consider this a way to define
a unit through all its exact behaviours towards its neighbours.

On the other hand, 'loose' mocks are looser. They allow both
unfullfilled expectations and try to handle unexpected calls in a
reasonable way.

You can use both with in the same suite of tests using the call
`cgreen_mocks_are(strict_mocks);` and `cgreen_mocks_are(loose_mocks);`
respectively.


==== Learning Mocks ====

Working with legacy code and trying to apply TDD, BDD or even simply
add some unit tests is not easy. You're working with unknown code that
does unknown things with unknown counterparts.

So the first step would be to isolate the unit. We won't go into
details on how to do that here, but basically you would replace the
interface to other units with mocks. This is a somewhat tedious manual
labor, but will result in an isolated unit where you can start
applying your unit tests.

Once you have your unit isolated in a harness of mocks, we need to
figure out which calls it does to other units, now replaced by mocks,
in the specific case we are trying to test.

This might be complicated, so *Cgreen* makes that a bit simpler. There
is a third 'mode' of the *Cgreen* mocks, the learning mocks.

If you temporarily add the call `cgreen_mocks_are(learning_mocks);` at
the beginning of your unit test, the mocks will record all calls and
present a list of those calls in order, including the actual parameter
values, on the standard output.

So let's look at the following example from the *Cgreen* unit
tests. It's a bit contorted since the test actually call the mocked
functions directly, but I believe it will serve as an example.

[source,c]
-----
include::tutorial_src/learning_mocks.c[lines=8..-1]
-----

We can see the call to `cgreen_mocks_are()` starting the test and
setting the mocks into learning mode.

If we run this, just as we usually run tests, the following will show
up in our terminal:

// This needs to be copied and pasted from the output of make in tutorial_src
// At least, I couldn't make make capture it into the output file
// It ought to be:
// include::tutorial_src/learning_mocks.out[]
----
Running "learning_mocks" (1 tests)...
LearningMocks -> emit_pastable_code : Learned mocks are
        expect(string_out, when(p1, is_equal_to(1)));
        expect(string_out, when(p1, is_equal_to(2)));
        expect(integer_out);
        expect(integer_out);
        expect(string_out, when(p1, is_equal_to(3)));
        expect(integer_out);
Completed "LearningMocks": 0 passes, 0 failures, 0 exceptions.
Completed "learning_mocks": 0 passes, 0 failures, 0 exceptions.
----

If this was for real we could just copy this and paste it in place of
the call to `cgreen_mocks_are()` and we have all the expectations
done.


Context, Subject Under Test & Suites
------------------------------------

As mentioned earlier, *Cgreen* promotes the behaviour driven
style of test driving code. The thinking behind BDD is that we don't
really want to test anything, if we just could specify the behaviour
of our code and ensure that it actually behaves this way we would be
fine.

This might seem like an age old dream, but when you think about it,
there is actually very little difference in the mechanics from
vanillla TDD. First we write how we want it, then implement it. But
the small change in wording, from `test¬¥ to `behaviour¬¥, from `test
that¬¥ to `ensure that¬¥, makes a huge difference in thinking, and also
very often in quality of the resulting code.

The SUT - Subject Under Test
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Since BDD talks about behaviour, there has to be something that we can
talk about as having the wanted behaviour. This is usually called the
SUT, the Subject Under Test. *Cgreen* in BDD-ish mode requires that you
define a name for it.

[source, c]
-----------------------
#include <cgreen/cgreen.h>
Describe(SUT);
-----------------------

*Cgreen* supports C++ and there you naturally have the objects and
also the Class Under Test. But in plain C you will have to think about
what is actually the "class" under test. E.g. in `sort_test.c` you might
see

[source, c]
---------------------
#include <cgreen/cgreen.h>
Describe(Sorter);

Ensure(Sorter, can_sort_an_empty_list) {
  assert_that(sorter(NULL), is_null);
}
---------------------

In this example you can clearly see what difference the BDD-ish style
makes when it comes to naming. Convention, and natural language,
dictates that typical names for what TDD would call tests, now starts
with 'can' or 'finds' or other verbs, which makes the specification so
much easier to read.

Yes, I wrote 'specification'. Because that is how BDD views what TDD
basically calls a test suite. The suite specifies the behaviour of a
`class¬¥. (That's why some BDD frameworks draw on 'spec', like
*RSpec*.)

Contexts and Before and After
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The complete specification of the behaviour of a SUT might become long
and require various forms of setup. When using TDD style you
would probably break this up into multiple suites having their own
`setup()` and `teardown()`.

With BDD-ish style we could consider a suite as a behaviour
specification for our SUT 'in a particular context'. E.g.

[source, c]
------------------------
#include <cgreen/cgreen.h>

Describe(shopping_basket_for_returning_customer);

Customer *customer;

BeforeEach(shopping_basket_for_returning_customer){
  customer = create_test_customer();
  login(customer);
}

AfterEach(shopping_basket_for_returning_customer) {
  logout(customer);
  destroy_customer(customer);
}

Ensure(shopping_basket_for_returning_customer, can_use_discounts) {
  ...
}
------------------------

The 'context' would then be `shopping_basket_for_returning_customer`,
with the SUT being the shopping basket 'class'.

So 'context', 'subject under test' and 'suite' are mostly
interchangable concepts in *Cgreen* lingo. It's a named group of
'tests' that share the same `BeforeEach` and `AfterEach` and lives in
the same source file.

[[changing_style, Changing Style]]

Changing Style
--------------

If you already have some TDD style *Cgreen* test suites, it is quite
easy to change them over to BDD-ish style. Here are the steps required

* Add `Describe(SUT);`

* Turn your current setup function into a `BeforeEach()` definition by
changing its signature to match the macro, or simply call the existing
setup function from the BeforeEach(). If you don't have any setup function
you still need to define an empty `BeforeEach()`.

* Ditto for `AfterEach()`.

* Add the SUT to each `Ensure()` by inserting it as a first parameter.

* Change the call to add the tests to `add_test_with_context()` by
  adding the name of the SUT as the second parameter.

* Optionally remove the calls to `set_setup()` and `set_teardown()`.

Done.

If you want to continue to run the tests using a hand-coded runner,
you can do that by keeping the setup and teardown functions and their
corresponding `set_`-calls.

It's nice that this is a simple process, because you can change over
from TDD style to BDD-ish style in small steps. You can convert one source
file at a time, by just following the recipe above. Everything will
still work as before but your tests and code will likely improve.

And once you have changed style you can fully benefit from the
automatic discovery of tests as described in <<runner, Automatic Test
Discovery>>.



[[reporter, Changing Cgreen Reporting]]

Changing Cgreen Reporting
-------------------------

Replacing the reporter
~~~~~~~~~~~~~~~~~~~~~~
            
In every test suite so far, we have run the tests with this line...

[source,c]
-----------------------
return run_test_suite(our_tests(), create_text_reporter());
-----------------------

We can change the reporting mechanism just by changing this
method.

Here is the code for `create_text_reporter()`...

[source,c]
-----------------------
TestReporter *create_text_reporter(void) {
    TestReporter *reporter = create_reporter();
    if (reporter == NULL) {
        return NULL;
    }
    reporter->start_suite = &text_reporter_start_suite;
    reporter->start_test = &text_reporter_start_test;
    reporter->show_fail = &show_fail;
    reporter->show_incomplete = &show_incomplete;
    reporter->finish_test = &text_reporter_finish;
    reporter->finish_suite = &text_reporter_finish;
    return reporter;
}
-----------------------

The `TestReporter` structure contains function pointers that control
the reporting.  When called from `create_reporter()` constructor,
these pointers are set up with functions that display nothing. The
text reporter code replaces these with something more dramatic, and
then returns a pointer to this new object. Thus the
`create_text_reporter()` function effectively extends the object from
`create_reporter()`.
             
The text reporter only outputs content at the start of the first test,
at the end of the test run to display the results, when a failure
occurs, and when a test fails to complete.  A quick look at the
`text_reporter.c` file in *Cgreen* reveals that the overrides just
output a message and chain to the versions in `reporter.h`.

To change the reporting mechanism ourselves, we just have to know a little
about the methods in the `TestReporter` structure.
             
The TestReporter structure
~~~~~~~~~~~~~~~~~~~~~~~~~~

The *Cgreen* `TestReporter` is a pseudo class that looks
something like...

[source,c]
-----------------------
typedef struct _TestReporter TestReporter;
struct _TestReporter {
    void (*destroy)(TestReporter *reporter);
    void (*start_suite)(TestReporter *reporter, const char *name, const int count);
    void (*start_test)(TestReporter *reporter, const char *name);
    void (*show_pass)(TestReporter *reporter, const char *file, int line,
                                   const char *message, va_list arguments);
    void (*show_fail)(TestReporter *reporter, const char *file, int line,
                                   const char *message, va_list arguments);
    void (*show_incomplete)(TestReporter *reporter, const char *file, int line,
                                   const char *message, va_list arguments);
    void (*assert_true)(TestReporter *reporter, const char *file, int line, int result,
                                   const char * message, ...);
    void (*finish_test)(TestReporter *reporter, const char *file, int line);
    void (*finish_suite)(TestReporter *reporter, const char *file, int line);
    int passes;
    int failures;
    int exceptions;
    void *breadcrumb;
    int ipc;
    void *memo;
    void *options;
};
-----------------------

The first block are the methods that can be overridden:


`void (*destroy)(TestReporter *reporter)`::

This is the destructor for the default structure. If this is
overridden, then the overriding function must call
`destroy_reporter(TestReporter *reporter)` to finish the clean up.


`void (*start_suite)(TestReporter *reporter, const char *name, const int count)`::

This is the first of the callbacks. At the start of
each test suite *Cgreen* will call this method on the reporter with
the name of the suite being entered and the number of tests in that
suite. The default version keeps track of the stack of tests in the
`breadcrumb` pointer of `TestReporter`. If you make use of the
breadcrumb functions, as the defaults do, then you will need to call
`reporter_start()` to keep the book keeping in sync.


`void (*start_test)(TestReporter *reporter, const char *name)`::

At the start of each test *Cgreen* will call this method on the
reporter with the name of the test being entered. Again, the default
version keeps track of the stack of tests in the `breadcrumb` pointer
of `TestReporter`. If you make use of the breadcrumb functions, as the
defaults do, then you will need to call `reporter_start()` to keep the
book keeping in sync.


`void (*show_pass)(TestReporter *reporter, const char *file, int line, const char *message, va_list arguments)`::

This method is initially empty as there most reporters see little
point in reporting passing tests (but you might do), so there is no
need to chain the call to any other function. Besides the pointer to
the reporter structure, *Cgreen* also passes the file name of the
test, the line number of failed assertion, the message to show and any
additional parameters to substitute into the message. The message
comes in as `printf()` style format string, and so the variable
argument list should match the substitutions.


`void (*show_fail)(TestReporter *reporter, const char *file, int line, const char *message, va_list arguments)`::

The partner of `show_pass()`, and the one you'll likely overload first.


`void (*show_incomplete)(TestReporter *reporter, const char *file, int line, const char *message, va_list arguments)`::

When a test fails to complete, this is the handler that is called. As it's an unexpected
outcome, no message is received, but we do get the name of the
test. The text reporter combines this with the breadcrumb to produce
the exception report.


`void (*assert_true)(TestReporter *reporter, const char *file, int line, int result, const char * message, ...)`::

This is not normally overridden and is really internal. It is the raw
entry point for the test messages from the test suite. By default it
dispatches the call to either `show_pass()` or `show_fail()`.


`void (*finish_test)(TestReporter *reporter, const char *file, int line)`::

The counterpart to the `(*start_test)()` call. It is called on leaving
the test. It needs to be chained to the `reporter_finish()` to keep
track of the breadcrumb book keeping.


`void (*finish_suite)(TestReporter *reporter, const char *file, int line)`::

The counterpart to the `(*start_suite)()` call called on leaving the
test suite, and similar to the `(*finish_test)()` if your reporter
needs a handle on that event too. The default text reporter chains
both this and `(*finish_test)()` to the same function where it figures
out if it is the end of the top level suite. If so, it prints the
familiar summary of passes and fails.


The second block is simply resources and book keeping that the reporter
can use to liven up the messages...

[horizontal]
`passes`:: The number of passes so far.
`failures`::  The number of failures generated so far.
`exceptions`:: The number of test functions that have failed to complete so far. 
`breadcrumb`:: This is a pointer to the list of test names in the stack.
				
The `breadcrumb` pointer is different and needs a little explanation.
Basically it is a stack, analogous to the breadcrumb trail you see on
websites.  Everytime a `start()` handler is invoked, the name is
placed in this stack.  When a `finish()` message handler is invoked, a
name is popped off.

There are a bunch of utility functions in `cgreen/breadcrumb.h` that
can read the state of this stack.  Most useful are
`get_current_from_breadcrumb()` which takes the breadcrumb pointer and
returns the current test name, and `get_breadcrumb_depth()` which gives
the current depth of the stack.  A depth of zero means that the test
run has finished.

If you need to traverse all the names in the breadcrumb, then you can
call `walk_breadcrumb()`.  Here is the full signature...

[source,c]
-----------------------
void walk_breadcrumb(Breadcrumb *breadcrumb, void (*walker)(const char *, void *), void *memo);
-----------------------

The `void (*walker)(const char *, void *)` is a callback that will be
passed the name of the test suite for each level of nesting.

It is also passed the `memo` pointer that was passed to the
`walk_breadcrumb()` call.  You can use this pointer for anything you
want, as all *Cgreen* does is pass it from call to call.  This is so
aggregate information can be kept track of whilst still being
reentrant.


The last parts of the `TestReporter` structure are...

[horizontal]
`ipc`:: This is an internal structure for handling the messaging between reporter
and test suite. You shouldn't touch this.

`memo`:: By contrast, this is a spare pointer for your own expansion.

`options`:: A pointer to a reporter specific structure that can be
used to set options. E.g. the textreporter defines the structure
`TextReporterOptions` which can be used by calling code to define the
use of colors when printing passes and failures. You set it with
`set_reporter_options(*void)`.
             
An example XML reporter
~~~~~~~~~~~~~~~~~~~~~~~
            
Let's make things real with an example.  Suppose we want to send the
output from *Cgreen* in XML format, say for storing in a repository or
for sending across the network.

NOTE: The `cgreen-runner` already has an XML-reporter that you can
use. See <<options, Cgreen Runner Options>>.

Suppose also that we have come up with the following format...

[source,xml]
-----------------------
<?xml?>
<suite name="Top Level">
    <suite name="A Group">
        <test name="a_test_that_passes">
        </test>
        <test name="a_test_that_fails">
            <fail>
                <message>A failure</message>
                <location file="test_as_xml.c" line="8"/>
            </fail>
        </test>
    </suite>
</suite>
-----------------------

In other words a simple nesting of tests with only failures encoded.
The absence of "fail" XML node is a pass.

Here is a test script, `test_as_xml.c` that we can use to construct the
above output...

[source,c]
-----------------------
include::tutorial_src/test_as_xml0.c[]
-----------------------

We can't use the auto-discovering `cgreen-runner` here since we need
to ensure that the nested suites are reported as a nested xml
structure. And we're not actually writing real tests, just something
that we can use to drive our new reporter.

The text reporter is used just to confirm that everything is
working. So far it is.

-----------------------
include::tutorial_src/test_as_xml0.out[]
-----------------------

Our first move is to switch the reporter from text, to our
not yet written XML version...

[source,c]
-----------------------
#include "xml_reporter.h"
...
include::tutorial_src/test_as_xml1.c[lines=24..-1]
-----------------------

We'll start the ball rolling with the `xml_reporter.h`
header file...

[source,c]
-----------------------
include::tutorial_src/xml_reporter.h[]
-----------------------

...and the simplest possible reporter in `xml_reporter.c`.

[source,c]
-----------------------
include::tutorial_src/xml_reporter0.c[]
-----------------------

One that outputs nothing.

-----------------------
$ gcc -c test_as_xml.c
$ gcc -c xml_reporter.c
$ gcc xml_reporter.o test_as_xml.o -lcgreen -o xml
$ ./xml
-----------------------

Yep, nothing.

Let's add the outer XML tags first, so that we can see *Cgreen*
navigating the test suite...

[source,c]
-----------------------
include::tutorial_src/xml_reporter1.c[]
-----------------------

Although chaining to the underlying `reporter_start()`
and `reporter_finish()` functions is optional, I want to
make use of some of the facilities later.

Our output meanwhile, is making its first tentative steps...

[source,xml]
-----------------------
include::tutorial_src/test_as_xml1.out[]
-----------------------

We don't require an XML node for passing tests, so the `show_fail()`
function is all we need...

[source,c]
-----------------------
...
include::tutorial_src/xml_reporter2.c[lines=18..25]
...
include::tutorial_src/xml_reporter2.c[lines=37..-1]
-----------------------

We have to use `vprintf()` to handle the variable argument list passed
to us.  This will probably mean including the `stdarg.h` header as
well as `stdio.h`.

This gets us pretty close to what we want...

[source,xml]				
-----------------------
include::tutorial_src/test_as_xml2.out[]
-----------------------

For completeness we should add a tag for a test that doesn't complete.
We'll output this as a failure, although we don't bother with the
location this time...

[source,c]
-----------------------
include::tutorial_src/xml_reporter3.c[lines=27..31]
...
include::tutorial_src/xml_reporter3.c[lines=44..-1]
-----------------------

All that's left then is the XML declaration and the thorny issue of
indenting.  Although the indenting is not strictly necessary, it would
make the output a lot more readable.

Given that the test depth is kept track of for us with the
`breadcrumb` object in the `TestReporter` structure, indentation will
actually be quite simple.  We'll add an `indent()` function that
outputs the correct number of tabs...

[source,c]
-----------------------
include::tutorial_src/xml_reporter4.c[lines=7..12]
-----------------------

The `get_breadcrumb_depth()` function just gives the current test
depth as recorded in the reporters breadcrumb (from
`cgreen/breadcrumb.h`).  As that is just the number of tabs to output,
the implementation is trivial.
            
We can then use this function in the rest of the code.  Here is the
complete listing...

[source,c]
-----------------------
include::tutorial_src/xml_reporter4.c[]
-----------------------

And finally the desired output...

[source,xml]
-----------------------
include::tutorial_src/test_as_xml4.out[]
-----------------------

Job done.

Possible other reporter customizations include reporters that write to
`syslog`, talk to IDE plug-ins, paint pretty printed documents or just
return a boolean for monitoring purposes.

Hints and Tips
--------------

CAUTION: This chapter is in its infancy. It will contain tips for
situations that you need some help to get out of.

Compiler Error Messages
~~~~~~~~~~~~~~~~~~~~~~~

Sometimes you can get cryptic and strange error messages from the
compiler. Since *Cgreen* uses some C/C++ macro magic this can happen
and the error messages might not be straight forward to interpret.

|=========================================================
|*Compiler error message*                    |*Probable cause...*
|`"contextFor<X>" is undeclared here`        |You forgot the `BeforeEach()` function
|`undefined reference to 'AfterEach_For_<X>'`|You forgot the `AfterEach()` function
|`CgreenSpec__<X>__<Y>__ is undeclared`      |You forgot to specify the test subject/context in a BDD style test
|=========================================================


Signed, Unsigned, Hex and Byte
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cgreen attempts to handle primitive type comparisons with a single
constraint, `is_equal_to()`. This means that it must store the actual
and expected values in a form that will accomodate all possible values
that primitive types might take, typically an `intptr_t`.

This might sometimes cause unexpected comparisons since all actual
values will be cast to match `intptr_t`, which is a signed value. E.g.

[source, c]
------------------------------
Ensure(Char, can_compare_byte) {
  char chars[4] = {0xaa, 0xaa, 0xaa, 0};
  assert_that(chars[0], is_equal_to(0xaa));
}
------------------------------

On a system which considers `char` to be signed this will cause the
following Cgreen assertion error:

------------------------------
char_tests.c:11: Failure: Char -> can_compare_byte
        Expected [chars[0]] to [equal] [0xaa]
                actual value:                   [-86]
                expected value:                 [170]
------------------------------

This is caused by the C rules forcing an implicit cast of the `signed
char` to `intptr_t` by sign-extension. This might not be what you
expected. The correct solution, by any standard, is to cast the actual
value to `unsigned char` which will then be interpreted correctly. And
the test passes.

NOTE: Casting to `unsigned` will not always suffice since that is
interpreted as `unsigned int` which will cause a sign-extension from
the `signed char` and might or might not work depending on the size of
`int` on your machine.

In order to reveal what really happens you might want to see the actual and
expected values in hex. This can easily be done with the
`is_equal_to_hex()`.

[source, c]
------------------------------
Ensure(Char, can_compare_byte) {
  char chars[4] = {0xaa, 0xaa, 0xaa, 0};
  assert_that(chars[0], is_equal_to_hex(0xaa));
}
------------------------------

This might make the mistake easier to spot:

------------------------------
char_tests.c:11: Failure: Char -> can_compare_byte
        Expected [chars[0]] to [equal] [0xaa]
        actual value:                   [0xfffffffffffffaa]
        expected value:                 [0xaa]
------------------------------



[appendix]
GNU Free Documentation License
------------------------------

----

    Version 1.1, March 2000

    
      Copyright (C) 2000  Free Software Foundation, Inc.
59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
Everyone is permitted to copy and distribute verbatim copies
of this license document, but changing it is not allowed.
    
    0. PREAMBLE

    The purpose of this License is to make a manual, textbook,
    or other written document "free" in the sense of freedom: to
    assure everyone the effective freedom to copy and redistribute it,
    with or without modifying it, either commercially or
    noncommercially.  Secondarily, this License preserves for the
    author and publisher a way to get credit for their work, while not
    being considered responsible for modifications made by
    others.

    This License is a kind of "copyleft", which means that
    derivative works of the document must themselves be free in the
    same sense.  It complements the GNU General Public License, which
    is a copyleft license designed for free software.

    We have designed this License in order to use it for manuals
    for free software, because free software needs free documentation:
    a free program should come with manuals providing the same
    freedoms that the software does.  But this License is not limited
    to software manuals; it can be used for any textual work,
    regardless of subject matter or whether it is published as a
    printed book.  We recommend this License principally for works
    whose purpose is instruction or reference.

    1. APPLICABILITY AND DEFINITIONS

    This License applies to any manual or other work that
    contains a notice placed by the copyright holder saying it can be
    distributed under the terms of this License.  The "Document",
    below, refers to any such manual or work.  Any member of the
    public is a licensee, and is addressed as "you".

    A "Modified Version" of the Document means any work
    containing the Document or a portion of it, either copied
    verbatim, or with modifications and/or translated into another
    language.

    A "Secondary Section" is a named appendix or a front-matter
    section of the Document that deals exclusively with the
    relationship of the publishers or authors of the Document to the
    Document's overall subject (or to related matters) and contains
    nothing that could fall directly within that overall subject.
    (For example, if the Document is in part a textbook of
    mathematics, a Secondary Section may not explain any mathematics.)
    The relationship could be a matter of historical connection with
    the subject or with related matters, or of legal, commercial,
    philosophical, ethical or political position regarding
    them.

    The "Invariant Sections" are certain Secondary Sections
    whose titles are designated, as being those of Invariant Sections,
    in the notice that says that the Document is released under this
    License.

    The "Cover Texts" are certain short passages of text that
    are listed, as Front-Cover Texts or Back-Cover Texts, in the
    notice that says that the Document is released under this
    License.

    A "Transparent" copy of the Document means a
    machine-readable copy, represented in a format whose specification
    is available to the general public, whose contents can be viewed
    and edited directly and straightforwardly with generic text
    editors or (for images composed of pixels) generic paint programs
    or (for drawings) some widely available drawing editor, and that
    is suitable for input to text formatters or for automatic
    translation to a variety of formats suitable for input to text
    formatters.  A copy made in an otherwise Transparent file format
    whose markup has been designed to thwart or discourage subsequent
    modification by readers is not Transparent.  A copy that is not
    "Transparent" is called "Opaque".

    Examples of suitable formats for Transparent copies include
    plain ASCII without markup, Texinfo input format, LaTeX input
    format, SGML or XML using a publicly available DTD, and
    standard-conforming simple HTML designed for human modification.
    Opaque formats include PostScript, PDF, proprietary formats that
    can be read and edited only by proprietary word processors, SGML
    or XML for which the DTD and/or processing tools are not generally
    available, and the machine-generated HTML produced by some word
    processors for output purposes only.

    The "Title Page" means, for a printed book, the title page
    itself, plus such following pages as are needed to hold, legibly,
    the material this License requires to appear in the title page.
    For works in formats which do not have any title page as such,
    "Title Page" means the text near the most prominent appearance of
    the work's title, preceding the beginning of the body of the
    text.

    2. VERBATIM COPYING

    You may copy and distribute the Document in any medium,
    either commercially or noncommercially, provided that this
    License, the copyright notices, and the license notice saying this
    License applies to the Document are reproduced in all copies, and
    that you add no other conditions whatsoever to those of this
    License.  You may not use technical measures to obstruct or
    control the reading or further copying of the copies you make or
    distribute.  However, you may accept compensation in exchange for
    copies.  If you distribute a large enough number of copies you
    must also follow the conditions in section 3.

    You may also lend copies, under the same conditions stated
    above, and you may publicly display copies.

    3. COPYING IN QUANTITY

    If you publish printed copies of the Document numbering more
    than 100, and the Document's license notice requires Cover Texts,
    you must enclose the copies in covers that carry, clearly and
    legibly, all these Cover Texts: Front-Cover Texts on the front
    cover, and Back-Cover Texts on the back cover.  Both covers must
    also clearly and legibly identify you as the publisher of these
    copies.  The front cover must present the full title with all
    words of the title equally prominent and visible.  You may add
    other material on the covers in addition.  Copying with changes
    limited to the covers, as long as they preserve the title of the
    Document and satisfy these conditions, can be treated as verbatim
    copying in other respects.

    If the required texts for either cover are too voluminous to
    fit legibly, you should put the first ones listed (as many as fit
    reasonably) on the actual cover, and continue the rest onto
    adjacent pages.

    If you publish or distribute Opaque copies of the Document
    numbering more than 100, you must either include a
    machine-readable Transparent copy along with each Opaque copy, or
    state in or with each Opaque copy a publicly-accessible
    computer-network location containing a complete Transparent copy
    of the Document, free of added material, which the general
    network-using public has access to download anonymously at no
    charge using public-standard network protocols.  If you use the
    latter option, you must take reasonably prudent steps, when you
    begin distribution of Opaque copies in quantity, to ensure that
    this Transparent copy will remain thus accessible at the stated
    location until at least one year after the last time you
    distribute an Opaque copy (directly or through your agents or
    retailers) of that edition to the public.

    It is requested, but not required, that you contact the
    authors of the Document well before redistributing any large
    number of copies, to give them a chance to provide you with an
    updated version of the Document.

    4. MODIFICATIONS

    You may copy and distribute a Modified Version of the
    Document under the conditions of sections 2 and 3 above, provided
    that you release the Modified Version under precisely this
    License, with the Modified Version filling the role of the
    Document, thus licensing distribution and modification of the
    Modified Version to whoever possesses a copy of it.  In addition,
    you must do these things in the Modified Version:

      Use in the Title Page
      (and on the covers, if any) a title distinct from that of the
      Document, and from those of previous versions (which should, if
      there were any, be listed in the History section of the
      Document).  You may use the same title as a previous version if
      the original publisher of that version gives permission.
      

      List on the Title Page,
      as authors, one or more persons or entities responsible for
      authorship of the modifications in the Modified Version,
      together with at least five of the principal authors of the
      Document (all of its principal authors, if it has less than
      five).
      

      State on the Title page
      the name of the publisher of the Modified Version, as the
      publisher.
      

      Preserve all the
      copyright notices of the Document.
      

      Add an appropriate
      copyright notice for your modifications adjacent to the other
      copyright notices.
      

      Include, immediately
      after the copyright notices, a license notice giving the public
      permission to use the Modified Version under the terms of this
      License, in the form shown in the Addendum below.
      

      Preserve in that license
      notice the full lists of Invariant Sections and required Cover
      Texts given in the Document's license notice.
      

      Include an unaltered
      copy of this License.
      

      Preserve the section
      entitled "History", and its title, and add to it an item stating
      at least the title, year, new authors, and publisher of the
      Modified Version as given on the Title Page.  If there is no
      section entitled "History" in the Document, create one stating
      the title, year, authors, and publisher of the Document as given
      on its Title Page, then add an item describing the Modified
      Version as stated in the previous sentence.
      

      Preserve the network
      location, if any, given in the Document for public access to a
      Transparent copy of the Document, and likewise the network
      locations given in the Document for previous versions it was
      based on.  These may be placed in the "History" section.  You
      may omit a network location for a work that was published at
      least four years before the Document itself, or if the original
      publisher of the version it refers to gives permission.
      

      In any section entitled
      "Acknowledgements" or "Dedications", preserve the section's
      title, and preserve in the section all the substance and tone of
      each of the contributor acknowledgements and/or dedications
      given therein.
      

      Preserve all the
      Invariant Sections of the Document, unaltered in their text and
      in their titles.  Section numbers or the equivalent are not
      considered part of the section titles.
      

      Delete any section
      entitled "Endorsements".  Such a section may not be included in
      the Modified Version.
      

      Do not retitle any
      existing section as "Endorsements" or to conflict in title with
      any Invariant Section.
      
    
    If the Modified Version includes new front-matter sections
    or appendices that qualify as Secondary Sections and contain no
    material copied from the Document, you may at your option
    designate some or all of these sections as invariant.  To do this,
    add their titles to the list of Invariant Sections in the Modified
    Version's license notice.  These titles must be distinct from any
    other section titles.

    You may add a section entitled "Endorsements", provided it
    contains nothing but endorsements of your Modified Version by
    various parties--for example, statements of peer review or that
    the text has been approved by an organization as the authoritative
    definition of a standard.

    You may add a passage of up to five words as a Front-Cover
    Text, and a passage of up to 25 words as a Back-Cover Text, to the
    end of the list of Cover Texts in the Modified Version.  Only one
    passage of Front-Cover Text and one of Back-Cover Text may be
    added by (or through arrangements made by) any one entity.  If the
    Document already includes a cover text for the same cover,
    previously added by you or by arrangement made by the same entity
    you are acting on behalf of, you may not add another; but you may
    replace the old one, on explicit permission from the previous
    publisher that added the old one.

    The author(s) and publisher(s) of the Document do not by
    this License give permission to use their names for publicity for
    or to assert or imply endorsement of any Modified Version.

    5. COMBINING DOCUMENTS

    You may combine the Document with other documents released
    under this License, under the terms defined in section 4 above for
    modified versions, provided that you include in the combination
    all of the Invariant Sections of all of the original documents,
    unmodified, and list them all as Invariant Sections of your
    combined work in its license notice.

    The combined work need only contain one copy of this
    License, and multiple identical Invariant Sections may be replaced
    with a single copy.  If there are multiple Invariant Sections with
    the same name but different contents, make the title of each such
    section unique by adding at the end of it, in parentheses, the
    name of the original author or publisher of that section if known,
    or else a unique number.  Make the same adjustment to the section
    titles in the list of Invariant Sections in the license notice of
    the combined work.

    In the combination, you must combine any sections entitled
    "History" in the various original documents, forming one section
    entitled "History"; likewise combine any sections entitled
    "Acknowledgements", and any sections entitled "Dedications".  You
    must delete all sections entitled "Endorsements."

    6. COLLECTIONS OF DOCUMENTS

    You may make a collection consisting of the Document and
    other documents released under this License, and replace the
    individual copies of this License in the various documents with a
    single copy that is included in the collection, provided that you
    follow the rules of this License for verbatim copying of each of
    the documents in all other respects.

    You may extract a single document from such a collection,
    and distribute it individually under this License, provided you
    insert a copy of this License into the extracted document, and
    follow this License in all other respects regarding verbatim
    copying of that document.

    7. AGGREGATION WITH INDEPENDENT WORKS
    
    A compilation of the Document or its derivatives with other
    separate and independent documents or works, in or on a volume of
    a storage or distribution medium, does not as a whole count as a
    Modified Version of the Document, provided no compilation
    copyright is claimed for the compilation.  Such a compilation is
    called an "aggregate", and this License does not apply to the
    other self-contained works thus compiled with the Document, on
    account of their being thus compiled, if they are not themselves
    derivative works of the Document.

    If the Cover Text requirement of section 3 is applicable to
    these copies of the Document, then if the Document is less than
    one quarter of the entire aggregate, the Document's Cover Texts
    may be placed on covers that surround only the Document within the
    aggregate.  Otherwise they must appear on covers around the whole
    aggregate.

    8. TRANSLATION

    Translation is considered a kind of modification, so you may
    distribute translations of the Document under the terms of section
    4.  Replacing Invariant Sections with translations requires
    special permission from their copyright holders, but you may
    include translations of some or all Invariant Sections in addition
    to the original versions of these Invariant Sections.  You may
    include a translation of this License provided that you also
    include the original English version of this License.  In case of
    a disagreement between the translation and the original English
    version of this License, the original English version will
    prevail.

    9. TERMINATION
    
    You may not copy, modify, sublicense, or distribute the
    Document except as expressly provided for under this License.  Any
    other attempt to copy, modify, sublicense or distribute the
    Document is void, and will automatically terminate your rights
    under this License.  However, parties who have received copies, or
    rights, from you under this License will not have their licenses
    terminated so long as such parties remain in full
    compliance.

    10. FUTURE REVISIONS OF THIS LICENSE

    The Free Software Foundation may publish new, revised
    versions of the GNU Free Documentation License from time to time.
    Such new versions will be similar in spirit to the present
    version, but may differ in detail to address new problems or
    concerns.  See http://www.gnu.org/copyleft/.

    Each version of the License is given a distinguishing
    version number.  If the Document specifies that a particular
    numbered version of this License "or any later version" applies to
    it, you have the option of following the terms and conditions
    either of that specified version or of any later version that has
    been published (not as a draft) by the Free Software Foundation.
    If the Document does not specify a version number of this License,
    you may choose any version ever published (not as a draft) by the
    Free Software Foundation.

    How to use this License for your documents

    To use this License in a document you have written, include
    a copy of the License in the document and put the following
    copyright and license notices just after the title page:


      Copyright (c)  YEAR  YOUR NAME.
      Permission is granted to copy, distribute and/or modify this document
      under the terms of the GNU Free Documentation License, Version 1.1
      or any later version published by the Free Software Foundation;
      with the Invariant Sections being LIST THEIR TITLES, with the
      Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.
      A copy of the license is included in the section entitled "GNU
      Free Documentation License".


    If you have no Invariant Sections, write "with no Invariant
    Sections" instead of saying which ones are invariant.  If you have
    no Front-Cover Texts, write "no Front-Cover Texts" instead of
    "Front-Cover Texts being LIST"; likewise for Back-Cover
    Texts.

    If your document contains nontrivial examples of program
    code, we recommend releasing these examples in parallel under your
    choice of free software license, such as the GNU General Public
    License, to permit their use in free software.

----
